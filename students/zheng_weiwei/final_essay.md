# **Fianl Essay**
## **Weiwei Zheng**

### **Individual Analysis** 
#### **Paper I**
**Summary of research design**
The authors utilized computational methods, a mixture of digital observation (counting and measuring things) and mass collaboration and tried to find out whether online marketplace facilitated or mitigated discrimination. By conducting a research on Airbnb, the popular online rental market platform, they discovered significant difference in charges received by black and non-black landlords, which implied evident racial discrimination as an unexpected outcome in sharing economy. 

One might believe that online market place helps create trust between buyers and sellers, for people can post information about sellers’ personal profiles, pictures, social media accounts and previous transaction review and history to establish reputation and credibility. However, the authors looked into the problem from a different angel and investigated on discrimination. They constructed a data set including pictures of landlords from New York on Airbnb together with information of their properties (property location, rental characteristics, review history, etc.).  

To label the race of landlords, the authors hired people from Amazon Mechanical Turk to code landlords’ racial identity from their profile pictures. The authors applied different models see whether landlords’ race is related to the price received by landlords after respectively controlling variables such as qualities of the apartment, property location, the quality of the apartment, consumer rating and social network presence to preclude the effect of covariates. Specifically, the authors also hired Amazon MTurk to rate the quality of each listing of rental information as measurement of the apartment’s quality. 

As result, the results showed an approximately twelve percent more rent received by non-black host than black host after holding other variables the same. Moreover, black hosts are incurred a larger price penalty for having a poor location score compared with their counterparts. This implies that the difference in rents mostly comes from taste discrimination instead of statistical discrimination. After people posted their personal information, such as profile picture, Internet actually increases racial discrimination. 

**Strength and weakness of the research design**
The most outstanding merit of the study is that it takes full advantage of the “big” of digital dataset. The authors collected data from the most popular online rental platform Airbnb, which stores rich resources of people’s online shopping and transaction record for properties rental in US. Given that Airbnb allows people to rent their whole properties or even a single bedroom for a pretty short time, it provides researchers with a representative window to observe the ecology of the whole online property rental market. 

Without such an enormous size of dataset, more than three thousand observations, researchers would have not recognized significant differences between subgroups. They can obtain much more robust results, draws more convincing causal inference, and detect more organized mechanism than the traditional quantitative study. 

The big data also benefits the study by allowing them to study rare events. In the dataset concerned, black people only occupy 7 percent of the whole population. Black people seem to be minority within those who have posted rental information of their apartments 

Moreover, due to the “big” of data, researchers also got able to study different subgroups. Though the authors didn’t sub-divide hosts’ race into more categories, thanks to the scale of the Airbnb dataset, researchers made it possible to specify the range of one singular variables. For example, they controlled effects of different number of bedrooms, different ranking of the apartment quality (in total a range of seven) and different kinds of social account available. By adding more variables into the model as a compound measurement of one dimension, the authors depicted more details in their causal inference. 

Another benefits of having digital dataset is its always-on characteristic. As a universally used application by apartment hosts and tenants, the dataset provides researchers with complete and real-time record of people’s transaction information. Researchers don’t need to worry about their might be any missing values or under-representative groups. 

The second strength is that researchers adopted mass collaboration to measure something hard for a small team of people to complete. In the study, people were hired from Amazon MTurk to categorize the racial attributes of hosts, as well as ranked the quality of apartment facilities. The method decomposed huge and demanding task into micro tasks, and then aggregate the results. 

However, the study still suffers some detrimental drawbacks. First of all, the authors didn’t consider the drifting characteristics of big data. Though by scraping information online can get researchers always-on research resources, the online rental platform actually keeps changing. Therefore, it would be better if the authors have conducted a longitudinal analysis. 

In addition, though they used mass collaboration in the study, the authors didn’t satisfy the principle of redundancy in human computation. General people’s categorization would be credible only if it was conducted several times. The authors didn’t state explicitly how many times each profile pictures and apartment facilities were reviewed by Amazon MTurk workers. If the process was only done once or twice it cannot get effective results.

Moreover, since researchers didn’t seem to cooperate with Airbnb to conduct the study, some doubts preserves over the algorithms of the platform by which they decide how much the price should be received by different hosts. There might be some unexpected factors influencing the price. If this had been the case, the authors could have given different weight to different variables in their models. 

Besides, authors neglected a significant covariate, the wording of apartment description. Personally speaking, the wording has a great influence on guests’ opinions on apartments as a part of promotion. Assuming that black people averagely have fewer education experience than white people, black hosts might be less able to write up fancy and engrossing description of their apartment to increase their properties’ popularity. 

Last but not least, the authors didn’t detect the heterogeneity of effects between different racial groups. Big data enables researchers to get enormous sample in each marginal cases. Some other groups inside the non-black category, say Asian, might have received even lower price than black hosts, while it probably had been overshadowed inside the larger non-black group. The racial discrimination is not a dichotomous dimension but a continuous spectrum, so comparison between smaller racial groups are quite unnecessary. 

#### **Paper II**
**Summary of research design**
In this computational study the author utilized digital experiment supplemented by observation. They tried to answer the question: Does discrimination exist in online marketplace? Airbnb allows hosts to decide whether to accept a guest or not after viewing her name and profile picture, which is presumed by the authors as a potential mechanism stimulating discrimination. The authors looked into the discrimination against hosts with different racial attributes. Besides, they also investigated whether certain type of hosts had more severe discrimination against black people than others. 

The main part of the research is a digital field experiment. The four components of the experiment: recruitment of participants, randomization of treatment, delivery of treatment and measurement of outcomes are all digital. The researchers collected data on all properties offered on Airbnb in Baltimore, Dallas, Los Angeles, St. Louis, and Washington, DC in July 2015. Then, they picked up 6,400 hosts and hired assistants from Amazon MTurk to categorize the race and gender of hosts by their profile picture posted online. After that, they gathered some information of different hosts such as how many properties they have, what kind of guests (race, gender) they accepted previously and information of each listing (more or less the same as the first paper). 

The experiment has four treatment groups. After coding the hosts, researchers created five accounts for each ofthe four groups. The account sent requests indicating different names of guests, representative of African American males, African American females, white males and white females to hosts. The requests contents and other information of the guests are controlled as the same, and theses psudo-accounts didn’t have a profile picture. The only way to detect the racial and gender identity of a guest is from their name. 

The response of the guests within one month were collected and coded as seven categories, from “Yes” to “No”. After controlling a list of confounders, such as characteristics of a list, apartment attributes, property desirability. The results show African American guests received a positive response roughly 42 percent of the time while those for white guests are around 50. And both male and female African American guests are discriminated against. But the rate of African Americans guests getting rejected by African American hosts is not significantly different from white hosts. Besides, discrimination were found be to concentrate among hosts with no previous experience of having black guests. 

To test the external validity of the research, an observation was also implemented to observe past review of different groups of hosts. The researchers found out discrimination gap between hosts without review from black guests and those with at least one review from black guests. 

**Strength and Weakness of the research design**
The research design obtained the study strong validity. First, as for statistical conclusion validity, the authors controlled effects of many covariates, as the first study did. Confounders including hosts’ gender, age, housing condition, interaction between race of guests and hosts, and quality of listing are all controlled to detect a valid causal relations between the rate of acceptance and the guests’ race. Second, the study holds strong construct validity. What they observe is what they care about. The name of guests is a valid construct of race, which is testified by an extra survey in the study. The difference rate in negligence towards different guest is an effective measurement of discrimination. Third, in terms of internal validity, the process of the experiment seems to go on smoothly. All of the four steps in traditional experiments are digitalized and accomplished by rigorous algorithms. Fourth, to acquire external validity, an extra observation/exploratory study was also conducted to generalize results to larger population. Researchers found out racial discrimination also exists in the real world by viewing reviews by different hosts, and the discrimination is mostly contributed by hosts never recruiting black people. 

The study also did a good job in measuring heterogeneity of effects. Thanks to big data, researchers achieved a much larger size of participants than in analog experiment, which enables them to observe trivial difference between different groups of participants. For example, people might assume racial identity of hosts and diversity of neighbourhood the hosts come from would lead to different extent of discrimination, but the experiment proves it wrong. By designing four treatment groups, the researchers got able to observe the interactive effect of race and gender. Without a considerable amount of participants in each of the four groups, it would have been impossible to notice different between groups. Thanks to big data, not only can they decompose the discrimination into different dimensions, but they can also preclude confounding effects by other variables and draw powerful conclusion from the study. 

The study is representative in observing mechanism in digital experiment. Due to the always-on feature of big data, the study can keep track of difference in a long time. The experiment compared the availability of certain list over certain period of time to measure its desirability and successfully precluded its effects. Mechanism looks at mediating and intervening variables working on the causal effect and explains how and why a phenomenon occurs. In lab experiment, we usually have a very short time to conduct the experiment due to the high expense. But the study can in contrast detect ties between different components in racial discrimination and attain robust results. 

As the first study, this one also supplemented the experiment with mass collaboration. People from Amazon MTurk were hired again to categorize the race and gender from different profile picture of the hosts. There are in total over six thousand hosts in the study, so it would be very difficult to categorize them in hand. However, though two people were asked to categorize each profile pictures, I still think the coding process is not credible enough. The principle of redundancy is fundamental in human computation, and only when a simple process is repeated many times can this method make sense. 

Again, big data benefits the study by enabling them to observe difference in different subgroups. In the study, there were both large amount of elements in both group, people never recruiting black people and those who have. 

However, despite that the research design of the second study is more robust than the first one the study still has some room to improve. In order to detect the inner mechanism of racial discrimination in online marketplace, it might not be enough for researchers to simply make treatment over guests’ names. Admittedly, name is an effective construct of race, but it might be not enough. Maybe there are colinearity between different racial name and other variable, say, popularity/familiarity of names. More dimensions over the race construct might be better. For example, the boot accounts can have profile picture or speak explicitly in the email about their race and gender. 

Ok, we need to mention ethic every time we talk about research study, though personally I don’t think there’s severe ethnic problematic involved in the study, as well as the first one. One might think in the second study, authors conducted deception, without getting informed consent from the participants. No, they didn’t but the harm to them is almost nothing, which can be definitely ignored. The results can get much larger benefits/inspiration in return. But there’s one thing they can improve: give some benefits back to participants after the study is done and debrief what they have done to them. As for the principle of justice, risk and benefits should be distributed fairly.

Last but not least, which applies to both study, if the authors want to examine the role Internet takes in increasing/decreasing discrimination, comparison with offline markets is necessary. Though the two studies both effectively demonstrate there’s significant racial discrimination in Airbnb, if the authors wants to pinpoint the negative effect of Internet on discrimination, the two studies are far from convincing. 

### **Compound Analysis**
**Value-add evaluation** 
The two studies complement each other. The core issue the authors get concerned are the same: Does Internet facilitate discrimination or not, and if yes, how? This research question cannot be answered by simply one of the research. As the authors stated in the literature review part of the two studies, online marketing platform such as Airbnb gives both guests and hosts great autonomy to decide whether to accept the request/offer of products or not. Therefore, it is an ideal platform to observe people’s consumption preference when limited information of products are provided. Discrimination is dichotomous. It wouldn’t have been a robust study if the authors had only examined one direction of discrimination. Only by the two research, can we get an in-depth understanding of the racial stratification inside online marketplace. 

The authors also combined observation and experiment to supplement each other. Observational studies inspect the issue of what it is and whether it exists or not, while experimental studies delve into how and why a phenomenon develops. If the authors had only run the observational study, there might be unobserved factors they ignored if they try to detect the causal relationship. Even the outcomes of the first study show price differences received by the two groups, never can we get one hundred percent sure what is left unconsidered in our model. However, by designing an experiment, researchers can control all the covariates the same between groups and set different treatment on randomized recruited participants. Experiment directly points out the causal relationship in the study. Other unobserved covariates are all precluded.

On the other hand, observational study also have its own merits. Digital observation tells people what they are concerned looks like and gives hints on how to conduct the following studies which further examine the causal effect. Besides, it’s relatively easy to conduct an observation study. Researchers don’t need to consider what treatment they need to control but just simply run models on everything they think interesting to detect inspiring relations. Induction research is more common in observation. Moreover, what is observed in observational study is more diverse, more closed to the real world context. For instance, when the authors are studying over the price changing of different groups, they could get a longitudinal dataset with much more variation. And if the researchers have got access to the whole dataset Airbnb, they could have more amplified response by different guests to different researchers. The relations between the two studies is like the observational and experimental study conducted by Gary King on Chinese censorship. Any one of them might be enough to show something, but two of them make the arguments more robust. 

And actually, due to the power of big data, researchers can finish the two studies at one time without increasing extra cost. In the first study, researchers coded the listing of apartments and profile picture of hosts which can be reused in the second study. Big data lower the cost to conduct large scale research than before. Conducting one single study might be hard, for researchers need to design powerful algorithms, clean dirty data and consider incomplete cases and system drift, but once the logistic process is done they can harvest much more benefits than expected and keep on following experiments. 

**Survey Research**
To answer the primary research question: Does discrimination exist in online marketplace? , a digital survey can be adopted to conduct enriched asking. The research questions can be reframed as follows: 1) Does racial discrimination exist in Airbnb? 2) What racial and gender group have more discrimination than others?

The steps of this research is as follows: First, randomly recruit a certain amount of posts in Airbnb websites. Second, each time a user is browsing theses property pages they would be asked several questions to rate the qualification/preference of the property in different aspects (decoration, convenience, facilities and willingness to stay). The users will be given some coupons back if they answer each of the questionnaire. Third, recruit people from Amazon MTurk to repeatedly answer the same set questions over same set of housing listings. Those users without a human profile picture will be excluded, and the profile picture of a list’s host won’t be shown when particpants are answering the question. Fourth, compare the answers from Airbnb users and AMT participants. We preclude the users who answer fewer than five questionnaires (the number is subject to change according to the statistical significance). And we calculate the differences of ratings of each users by comparing them with AMT participants. Divide the users into two groups: discriminating group and non-discriminating group. The discriminating group have ratings significantly different from AMT’s ratings while non-discriminating group do not. If the difference between users and AMT participants is significance, then discrimination exists. Fifth, recruit participants from Amazon MTurk again to repeatedly code the gender and race of people’s profile pictures in discriminating group and non-discriminating group, and then compare their gender and racial differences to see whether a specific group are more likely to have discrimination against hosts than others. 

The research design can help researchers explore the attitudes of users who do have discrimination when they viewing similar posts by hosts of different races. However, it still has some drawbacks. First of all, the design is only workable if Airbnb allows researchers to cooperate with them. There’s no way for people to distribute questionnaires if the company is not allowed. In addition, the research risks a low response rate. In order to avoid representation error, researchers need to reach a wide range of people representative of the general population, otherwise the estimation of their attributes would become invalid. Besides, if only a low proportion of users we analyze have recognizable profile pictures, it would also lead to representation errors. Apart from that, we need to consider the ethical question involved if we get the profile pictures of users without permission, though if we ask for their informed consent, it’s highly possible that they are not willing to take part in the study. Last, given that we are going to preclude those only answering a few amount of questions, it’s hard to determine who to include in the group for analysis. If the threshold of the amount of questionnaire completed is quite high to get representative sample, the measurement might possibly be biased. 

To compensate the drawbacks, several remedies can be made. First of all, give incremental rewards to users who are browsing the housing lists to increase response rate. Second, use matching, this non-probability sampling method to recruit questionnaires from Airbnb. Don’t stop the sampling process until the front end of all interest groups are reached. Third, anonymized the dataset. Try the best to encrypt users’ information and profile picture and avoid any potential harm that would be incurred on them. Forth, utilize post-stratification to deal with the unbalanced proportion of different subgroups. 

