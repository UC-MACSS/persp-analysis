#Ethics Paper
##Rita Biagioli
	
>It is quite obvious that the Facebook contagion study caused a significant stir in relation to ethics. However, most IRBs would readily either approve this study or consider it exempt; the goal of the IRB is to determine whether a study would harm participants in any way beyond that which is within the vicissitudes of everyday life. It is my contention that this study certainly falls within ethical guidelines, and, perhaps, indicates the way in which much social science research will be conducted going forward.

>Much research manipulates an environment in order to make some conclusion. Even anthropological research, for example, often requires that the anthropologist be present and thus, affect the environment in some way. One of the main issues of contention with the Facebook contagion study is that it affected mood through curating what users saw; an anthropologist might do the same by asking questions, or merely by being present. The question becomes: is mood a significantly disruptive factor in most people’s lives, and, if so, what magnitude of alteration is within the confines of what would happen normally within daily life?  This brings us to beneficence: how much _potential_ harm is in fact harm?
	
>I would argue that, in relation to this study, no unusual harm occurred. On a spectrum, the manipulation of positive or negative status updates was not outside of the realm of possibility and, indeed, it’s not as if the research team manufactured status updates; they purely altered what one saw. Moreover, while the study did correlate positive status updates as resulting in the posting of more positive status updates, and negative statuses with more negative updates later, _there is little argument to be made about what this means beyond contagion._ In so far as people are pattern observers and generators (which much research about the brain and otherwise would support), people tend to mimic each other. It’s a well known bit of trivia that people solve crossword puzzles more easily the day after they’re released; the reason being because the data from those puzzles seeps into our daily lives and affects us without us ever consciously knowing. Without gathering data about the particular emotional states of those individuals involved, there is no way to tell the true effect of the manipulation aside from changing the content of future status updates. 
	
>Small manipulations affect our days constantly, including the moods of others around us, which we cannot control. As I sit writing this, a man opposite me in this coffee shop just yelled at a woman in a way I would almost deem to be abusive. Does this affect my mood? Certainly. Would a shooting (all too frequent lately), or some natural disaster? Even the election? Of course. Thus, there’s an argument to be made that when considering these issues, we should think of them in comparison: perhaps we should address factors that, due to their magnitude, are significantly more likely to be harmful rather than minute alterations to one’s environment that one does not even actively observe. This is not to say that these might not harm someone, but rather, that there are significantly more harmful events in the world, far exceeding this potential harm in magnitude. So, did this study ultimately cause a significant amount of harm? I would conclude that it certainly did not cause harm beyond the usual potential for harm in daily life. It did not necessarily minimize potential harm for individuals, but did indeed maximize potential benefits in a societal way, in so far as research always does. To this end, I would deem it as within the confines of beneficence.
	
>One further point with relation to harm is that humans are notoriously difficult to predict. Though we can run studies to discern the overall trajectory of reactions, smaller population segments may not react in the same ways as does the aggregate. For example, as I have seen among many friends who are are nearing thirty and unmarried, announcements of engagements and weddings on Facebook cause significant anguish. Though the aforementioned study would likely deem these posts to be positive, for this particular group, they are excruciating. To this end, the scope of the effect of potential harm becomes important: in aggregate, there is little harm. For particular individuals or population segments, this might not be the case. Thus, the only way to truly fall within the bounds of justice is to select subjects randomly, omitting particularly at risk groups if possible, and know that there will be a continuum of experience. If the harm quotient is low enough for the aggregate overall, one has thus done all one can to ensure equal treatment.
	
>One particularly relevant example of protecting certain groups (or failing to) is the email UChicago sent out to incoming first years regarding trigger warnings. Though the email overall was perhaps unnecessarily harsh, it brings to light a conundrum: what will trigger someone is very difficult to predict, and that person may or may not be aware him or herself. There are obviously some trigger warnings that are predictable and important, such as those regarding sexual violence, because we have significant evidence that this kind of material can really cause additional harm. However, it is impossible to account for everything that could possibly trigger someone, which is, in some way, the point UChicago was making. Recently, I was listening to a Malcolm Gladwell podcast wherein he played a real 911 tape of a fatal car crash. I had not predicted that it would affect me in the way it did, but I was certainly upset afterwards. Would I have predicted that I would feel that way? Not necessarily. And, in fact, I think about that podcast fairly frequently; I learned something from the podcast, and also learned something about my own emotional reactions. While this is merely an analogy, the argument holds: we can try to protect every research subject from harm, and we can protect the aggregate and particularly vulnerable populations, but we cannot account for each particular individual’s particular background and proclivities.
	
>Moving on to discuss respect for law and public interest, it is certainly clear that this research team attempted to act in an ethical manner, and has been incredibly transparent about their methods, in fact, potentially to the end of provoking the uproar that occurred in reaction to their study. They did consider how to involve an IRB and reported on that. Ultimately, as researchers, they have done all the due diligence they can.
	
>In terms of respect for persons, this study does find itself on thin ice. However, this relates to a larger issue about how we use the internet, and this issue is only beginning to be addressed: Facebook, itself, is a company. When we use a company’s services, we do opt in to user agreements and acknowledge that, often in exchange for using a free service, whatever we do on that site is able to be accessed by the company. We, as users and not partial owners, have no control of what research the company might do or what deals it might make, so long as those are in compliance with the user agreement. To this end, while consent might not be fully informed, that is in some ways the result of a changing society where people need to be more cognizant of what information they put into the world.

>I have no qualms using this type of data in my own research, for the reasons outlined above. I find it telling that most of our class similarly had few qualms; in research, this is often par for the course. What I worry about more in terms of data gathered this way is that the dependent variable is less explicit than it seems; how much can we truly conclude from what are effectively correlations (even if they are manipulated), with no explicit measure of explanation? Such is the case with much data that could be potentially gathered online, unless users very intentionally opt in and are willing to complete scales as dependent measures.

>It is my conjecture that research that is funded by companies is truly the way of the future. Grants in academia are drying up, as are tenure-track positions. Response rates for surveys and other types of social science research are abysmal. However, the amount of data in the world is doubling every year. The world is changing, and information is more freely disseminated online. Unfortunately, research funded by companies obviously has its own bias and objectives, but, in some cases, it might be more advantageous for an academic to join with a company in order to first, have access to data that people are actually giving away, as per user agreements, and second, to ensure that there is funding for said research and that said research will occur efficiently. This type of research is not going away anytime soon, and it is the responsibility of the researcher in question to make sure said research is ethical. However, it is the responsibility of the populace to realize that their actions in the world, and particularly online, are no longer private and always have been, to some extent, observable.


https://www.caida.org/publications/papers/2012/menlo_report_actual_formatted/menlo_report_actual_formatted.pdf

http://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/

Rosen, J. (2014). Facebook's controversial study is business as usual for tech companies but corrosive for universities. The Washington Post.

Kramer, A. D., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. PNAS, 111(24), 8788-8790.

