---
title: "Unsuperized Learning Assignment"
author: "Weiwei Zheng"
date: "December 3, 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load data
```{r}
rm(list=ls())
library(readr)
library(ggplot2)
library(ggfortify)
College <- read_csv("~/persp-analysis/assignments/unsupervised-learning/data/College.csv")
College.data <- College[,-1,drop=FALSE] 
USArrests.data <- read_csv("~/persp-analysis/assignments/unsupervised-learning/data/USArrests.csv")
```

## PART A

**1. Perform PCA analysis on the college dataset and plot the first two principal components. Describe the results.**

```{r}
pr.college<-prcomp(College.data, scale = TRUE) 
biplot(pr.college, scale = 0, pch = 0.2)
```

```{r}
autoplot(pr.college)
```

```{r}
theta <- seq(0,2*pi,length.out = 100)
circle <- data.frame(x = cos(theta), y = sin(theta))
p <- ggplot(circle,aes(x,y)) + geom_path()
loadings <- data.frame(pr.college$rotation, 
                       .names = row.names(pr.college$rotation))
p + geom_text(data=loadings, 
              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +
  coord_fixed(ratio=1) +
  labs(x = "PC1", y = "PC2")
```

(The data needs scale = TRUE otherwise the result would be very hard to read and become meaningless. I did what is suggested by the textbook, which I think is more credible.) The first principal component is associated with variables including "Grad.rate", "Top20perc", "Top25perc", "Phd", "per.alumini", "Expend" and "Terminal", which stores the ahievement and prestige of colleges. The second principal component is associated with "F.undergrad", "P.undergrad", "Accept" and "Enroll", which stores information of current performance of schools.

**2. Calculate the cumulative proportion of variance explained by all the principal components (see 10.2.3 in ISLR). Approximately how much of the variance in College is explained by the first two principal components?**

```{r}
summary(pr.college)
```

PC1 explained 32 percent of variance in College, while PC2 explained 26.4 percent. In total 58.4 percent. 

## PART B

**1. Perform PCA on the dataset and plot the observations on the first and second principal components.**

```{r}
row.names(USArrests.data)<-USArrests.data$State
pr.arrest <- prcomp(USArrests.data[, 2:5], scale = TRUE) 
autoplot(pr.arrest, data = USArrests.data, label = TRUE, loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
```
```{r}
theta <- seq(0,2*pi,length.out = 100)
circle <- data.frame(x = cos(theta), y = sin(theta))
p <- ggplot(circle,aes(x,y)) + geom_path()
loadings <- data.frame(pr.arrest$rotation, 
                       .names = row.names(pr.arrest$rotation))
p + geom_text(data=loadings, 
              mapping=aes(x = PC1, y = PC2, label = .names, colour = .names)) +
  coord_fixed(ratio=1) +
  labs(x = "PC1", y = "PC2")
```

**2.Perform $K$-means clustering with $K=2$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.**

```{r}
set.seed(2)
km2.out <- kmeans(USArrests.data[2:5], 2, nstart =20)
km2.clusters <- km2.out$cluster
autoplot(pr.arrest, data = USArrests.data, label = TRUE, col = km2.clusters, loadings = TRUE, loadings.colour = 'blue',loadings.label = TRUE, loadings.label.size = 3)
```
(Order from left to right)
The first component mostly stores information about murder, assault and rape, with California, Nevada, Florida highly positively associated with it and Vermont, North Dakota, Maine, Iowa, New Hampshire negatively. The second component mostly stores information about urban population, with New Jersey, Massachusetts, Hawaii, and Rhode Island highly positively associated, and West Virginia, South Carolina, North Carolina and Mississippi negatively associated with it. The first cluster mostly includes states with high crime rate while the second low crime rate.

**3.Perform $K$-means clustering with $K=4$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.**

```{r}
km4.out <- kmeans(USArrests.data[2:5], 4, nstart =20)
km4.clusters <- km4.out$cluster
autoplot(pr.arrest, data = USArrests.data, label = TRUE, col = km4.clusters, loadings = TRUE, loadings.colour = 'blue',loadings.label = TRUE, loadings.label.size = 3)
```

The firs cluster includes states of high crime rate. The second cluster includes states of medium high crime rate and medium population size. The third cluster includes states of low crime rate and medium population size. The fourth group includes states of low crime rate and medium low population size.

**4.Perform $K$-means clustering with $K=3$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.**

```{r}
km3.out <- kmeans(USArrests.data[2:5], 3, nstart =20)
km3.clusters <- km3.out$cluster
autoplot(pr.arrest, data = USArrests.data, label = TRUE, col = km3.clusters, loadings = TRUE, loadings.colour = 'blue',loadings.label = TRUE, loadings.label.size = 3)
```
The first clustering includes states with high crime rate. The second clustering inlcudes states with medium crime rate and medium population size. The third inclues states with low crime rate and medium population size. 

**5.Perform K-means clustering with K=3 on the first two principal components score vectors, rather than the raw data. Describe your results and compare them to the clustering results with K=3 based on the raw data.**
```{r}
km3.out.new <- kmeans(pr.arrest$x[, 1:2], 3, nstart = 20)
km3new.clusters <- km3.out.new$cluster
autoplot(pr.arrest, data = USArrests.data, label = TRUE, col = km3new.clusters, loadings = TRUE, loadings.colour = 'blue',loadings.label = TRUE, loadings.label.size = 3, main = "K-means of 3 on First Two Score Vectors ")
```
```{r}
table(km3.clusters,km3new.clusters)
```

The boundaries of each clustering by the two methods are more or less the same, with the clustering on pc vector scores a bit more seperate. However, from the table, we can see the clusterings are different.  

**6.Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.**
```{r}
arrest.dist <- dist(USArrests.data)
plot(hclust(arrest.dist), labels = USArrests.data$State , main =" Complete Linkage ", xlab ="", sub ="", ylab ="")
```
**7.Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?**

```{r}
hc.out<-hclust(arrest.dist)
hc.clusters <- cutree(hc.out,3) 
hc.clusters
```
The grouping is displayed above.

**8.Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation $1$. What effect does scaling the variables have on the hierarchical clustering obtained?**

```{r}

sd.arrest <- scale(USArrests.data[, 2:5])
arrest.dist.scale <- dist(sd.arrest)
hc.out.scale<-hclust(arrest.dist.scale)
hc.clusters.scale <- cutree(hc.out.scale, 3)
table(hc.clusters.scale, hc.clusters)

```

The clusters obtained by the two methods are not identical. "Murder", "Assault" and "Rape" are measured as number of people in the unit of 100,000, while "UrbanPop" is the percentage of urban population, which leads to different results calculated by Euclidean distance.